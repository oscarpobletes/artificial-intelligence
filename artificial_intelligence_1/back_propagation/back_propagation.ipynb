{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Back propagation algorithm\n",
        "Óscar Poblete Sáenz <br> Course: Introduction to Artificial Intelligence<br>\n",
        "Teacher: Elizabeth Guevara Martinez<br>\n",
        "Universidad Anáhuac <br>"
      ],
      "metadata": {
        "id": "2OBTXtkacfjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code that implements the backpropagation algorithm and solves the two-input XOR problem."
      ],
      "metadata": {
        "id": "DEmHiA9ddCIb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wbwfE8DOa-29"
      },
      "outputs": [],
      "source": [
        "import numpy as np # Library for vectors and matrices with aliases np\n",
        "def sigmoid(a): # Activation function\n",
        "  return 1/(1+np.exp(-a))\n",
        "\n",
        "def Dsigmoid(a): # Derivative of the activation function\n",
        "  return a*(1-a)\n",
        "\n",
        "def feedForward(input, weights, biasWeights): # Information moves in only one direction, forward.\n",
        "  a=np.dot(input,weights)\n",
        "  a+=biasWeights # Neuron activation is calculated as the weighted sum of the inputs and the aggregation of the bias weights.\n",
        "  y= sigmoid(a) # Application of the sigmoid\n",
        "  return y\n",
        "\n",
        "# Updating weights using gradients to minimize error\n",
        "def updateWeights(weights, biasWeights, delta, deltaT, alfa):\n",
        "  weights=weights-delta*alfa \n",
        "  biasWeights=biasWeights-np.sum(deltaT)*alfa\n",
        "  return weights, biasWeights\n",
        "\n",
        "# Generation of H-Hidden O-Output random weights according to the topology\n",
        "def randomWeights(inputLayer, hiddenLayer, outputLayer):\n",
        "  H_weights = np.random.uniform(size=(inputLayer,hiddenLayer))\n",
        "  H_biasWeights =np.random.uniform(size=(1,hiddenLayer))\n",
        "  O_weights = np.random.uniform(size=(hiddenLayer,outputLayer))\n",
        "  O_biasWeights = np.random.uniform(size=(1,outputLayer))\n",
        "  return H_weights, H_biasWeights, O_weights, O_biasWeights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = np.array([[0,0],[0,1],[1,0],[1,1]]) # Two inputs\n",
        "output = np.array([[0],[1],[1],[0]]) #XOR\n",
        "epochs = 10000 # Epochs number\n",
        "alfa = 0.25 # Learning rate (amount of weight update during training)\n",
        "\n",
        "#Definición de la topología de la red\n",
        "inputLayer, hiddenLayer, outputLayer = 2,2,1\n",
        "\n",
        "# Random initialization of weights, H-Hidden, O-Output\n",
        "H_weights, H_biasWeights, O_weights, O_biasWeights=randomWeights(inputLayer, hiddenLayer, outputLayer)\n",
        "\n",
        "# Now, we are ready to do the forward and backward step calculations for a number of epochs. \n",
        "# Loop that goes through the previously established number of epochs.\n",
        "for i in range(0, epochs):\n",
        "    H_output=feedForward(input, H_weights, H_biasWeights) # Result of hidden nodes\n",
        "    O_output=feedForward(H_output,O_weights, O_biasWeights) # Result of output nodes\n",
        "    deltaOT=-(output-O_output)*Dsigmoid(O_output) # Delta is the gradient that minimizes the error\n",
        "    deltaO=np.dot(H_output.T, deltaOT) # .T generates the transpose of the array\n",
        "    deltaH1=np.dot(deltaOT,O_weights.T) \n",
        "    deltaH2=deltaH1*Dsigmoid(H_output)\n",
        "    deltaH=np.dot(input.T,deltaH2) \n",
        "    [O_weights, O_biasWeights]=updateWeights(O_weights, O_biasWeights, deltaO, deltaOT, alfa) # Update output layer weights\n",
        "    [H_weights, H_biasWeights]=updateWeights(H_weights, H_biasWeights, deltaH, deltaH2, alfa) # Update hidden layer weights\n",
        "\n",
        "#Impresión de resultados\n",
        "print(\"Final hidden layer weights:\\n \", H_weights, \"\\n \")\n",
        "print(\"Final hidden layer bias weights:\\n \", H_biasWeights, \"\\n \")\n",
        "print(\"Output layer final weights:\\n \",O_weights, \"\\n \")\n",
        "print(\"Output layer final bias weights:\\n \",O_biasWeights, \"\\n \")\n",
        "print(\"MLP output:\\n \",O_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrXiNsYqSMyK",
        "outputId": "1b0d0df3-0bfa-4d3c-e2d3-c9a000f15745"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final hidden layer weights:\n",
            "  [[3.55595994 8.4669146 ]\n",
            " [3.55598713 8.46701806]] \n",
            " \n",
            "Final hidden layer bias weights:\n",
            "  [[-5.30770249 -5.4520142 ]] \n",
            " \n",
            "Output layer final weights:\n",
            "  [[-10.56052671]\n",
            " [  9.05983633]] \n",
            " \n",
            "Output layer final bias weights:\n",
            "  [[-3.48768934]] \n",
            " \n",
            "MLP output:\n",
            "  [[0.02928389]\n",
            " [0.97307314]\n",
            " [0.97307299]\n",
            " [0.02943113]]\n"
          ]
        }
      ]
    }
  ]
}